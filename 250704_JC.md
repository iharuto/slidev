---
title: "Bioinfo Lab JC" 
lang: en 
theme: default 
paginate: true 
pageNum: true
---


## EEGPT: UNLEASHING THE POTENTIAL OF EEG GENERALIST FOUNDATION MODEL BY AUTOREGRES- SIVE PRE-TRAINING<br><br>
### EEG foundation model using Autoregressive Pre‚ÄëTraining

First author: Tongtian Yue$^{1,2}$  
Corresponding author: Jing Liu$^{1,2}$  
Affiliation: Zidongtaichu Foundation Model Research Center$^1$ & School of Artificial Intelligence, University of Chinese Academy of Sciences$^2$


---

## Table of Contents<br><br>

1. Background & Problem Setting
2. Novelty of This Study
3. Proposed Method (ETE & TEG) and materials
4. Experimental Results 1 to 4
5. Discussion & Limitations
6. Conclusion


---
layout: two-cols
---

## 1a. What is EEG? üß†„ÄÄ„Ä∞Ô∏é„ÄÄ„Ä∞Ô∏é

- It is the abbreviation of electroencephalogram
- Non‚Äëinvasive measurement of brain electrical activity using **scalp electrodes**
- Applied for estimating emotion / motor imagery / mental workload / sleep‚Äëstage


## 2a. Novelty of This Study ‚ú®

- **Autoregressive (AR) pre‚Äëtraining** applied to EEG for the first time
- 1‚ÄØB+ tokens (signal embeddings) used to verify **scaling laws**
- **Task‚Äëshared Electrode Graph (TEG)** allows transfer to 5 tasks √ó 12 benchmarks **with a single model**

::right::

## 1b. Challenges in Research ‚ö†Ô∏è

- **Heterogeneous experiment formats** (number & placement of electrodes, etc.)
- **MAE‚Äëstyle preprocessing** dominates and struggles to capture temporal dependency
- Existing models are **task‚Äëspecific**


## 2b. Outcomes üéØ

- **EEGPT‚ÄëGiant (1.09‚ÄØB params)** surpasses specialised models by **5‚Äì11‚ÄØ%** ‚Üí Result¬†1
- Demonstrated scaling laws: larger model size & more data improve performance ‚Üí Result¬†2
- Robust AR performance against unexperienced EEG data ‚Üí Result¬†4


---

## üèó 3a. Overall Architecture<div style="margin-top: 0.5em;"></div>
- **ETE**: Autoregressive pre‚Äëtraining for each electrode‚Äôs time series (leftside the architecture)
- **TEG**: Learns spatial crostalks between electrodes and performs multi‚Äëtask **transfer fine‚Äëtuning** (rightside)  

<div style="position: relative; display: inline-block;">
  <p align="center">
    <img src="./pdf/hoge/fig3.png" style="width: 85%;">
  </p>
  <div class="text-xs text-gray-500" style="position: absolute; bottom: 30px; right: 0px;">
    Figure. 3
  </div>
</div>
‚ö†Ô∏è Datasets were split into 8:1:1 for train/val/test. Results were obtained using 5 random seeds.

---

## 3b. Electrode Temporal Encoder (ETE): Ê¶ÇË¶Å<br><br>
| *Objective* | Obtain EEG representations involving chronological information |
|:--:|:--:|
| *Input* | 1‚Äësecond EEG window $x$ per electrode |
| *Tokenised Input* | Embedded vectors $X$ (electrode labels + signals concatenation) |
| _Q K V_ | $\mathbf{Q}=\mathbf{X}W_Q,\; \mathbf{K}=\mathbf{X}W_K,\; \mathbf{V}=\mathbf{X}W_V$ |
| _Attention_ | $\mathrm{softmax}\Bigl(\tfrac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}} + M\Bigr)\mathbf{V}$ |
| _Loss_ | $\mathcal{L}=\tfrac{1}{T}\sum_{t=1}^{T}(x_t-\hat{x}_t)^2$ Ôºà$MSE$Ôºâ|

> **Simple Example**\
> Input a 4‚Äëdimensional series `[0.1, 0.0, ‚àí0.2, ?]` and predict the last point ‚Äú?‚Äù.\
> Mask $M$ enforces that each timestep refers **only to the past**, enabling autoregressive representation learning.


---

## 3b. Electrode Temporal Encoder (ETE) simple ver<div style="margin-top: 0.5em;"></div>
- **Input**: EEG signals forming each electrode ($E$) √ó 1‚ÄØs window ($C$) √ó time series ($T$) 
- **Objective**: **Per‚Äëelectrode** autoregressive prediction through capturing temporal evolution implemented with a causal attention mechanism
<br>
> **Causal attention $\alpha$**:<br>‚ÄÉ$\displaystyle \alpha_{t,s}=\mathrm{softmax}\!\bigl(\tfrac{\mathbf{q}_t\mathbf{k}_s^\top}{\sqrt{d}}+M_{t,s}\bigr)$ Ôºà$M_{t,s}=-\infty$ if $s>t$Ôºâ
<br>
<div style="text-align:center;">
$$
\underbrace{\begin{bmatrix}q_1\\ q_2\\ q_3\end{bmatrix}}_{\mathbf{Q}}
\;\times\;
\underbrace{\begin{bmatrix}k_1^\top & k_2^\top & k_3^\top\end{bmatrix}}_{\mathbf{K}^\top}
=
\begin{bmatrix}
q_1k_1^\top & q_1k_2^\top & q_1k_3^\top \\
q_2k_1^\top & q_2k_2^\top & q_2k_3^\top \\
q_3k_1^\top & q_3k_2^\top & q_3k_3^\top
\end{bmatrix}
\Rightarrow \;\mathrm{softmax}_{\text{row}} \;\Rightarrow \mathbf{\alpha}
$$
</div>
<div style="text-align:center;">
$$
\displaystyle \mathbf{z}_t=\sum_{s\le t}\alpha_{t,s}\mathbf{v}_s
$$
</div>

> **Meaning of Œ±**: $\alpha_{t,s}$ is the weight indicating ‚Äúhow much the Query at time *t* attends to past step *s*‚Äù. If $q_t$ seeks a ‚Äúsmall‚Äëamplitude EEG pattern‚Äù and $k_s$ contains it, their inner product increases, $\alpha_{t,s}$ grows, and time *t* attends to *s*.  
> **Weighted Update**: $z_t$ merges past information into the token representation.


---
layout: two-cols
---

## 3c. Overview of TEG<br><br>

- **Purpose**: Utilise tokens condensed with past signal information, adding spatial context

- **Input**: __Last__ time‚Äëseries features $z_j \in \mathbb{R}^{E_j \times C}$ extracted by ETE

- **Processing**: **Graph attention** ‚Äì learn inter‚Äëelectrode relationships (go next slide)

- **Output**: Tokens with spatiotemporal information ‚Üí Task‚Äëspecific heads (classification / regression)

- **Loss**: MSE

::right::

<br><br>

![](./pdf/hoge/LLM_nyumon_fig3_2.png)

<div class="text-xs text-gray-500 mt-2 text-right">
Âõ≥3.2 - Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ „Çà„Çä
</div>


---
layout: two-cols
---

## 3c. TEG Overview <div style="margin-top: 0.5em;"></div>
### Graph Attention <span style="font-size: 70%;">(modified from Eq.¬†9)</span>
  1. Map the time‚Äëseries features $h_m$ and $h_n$ of electrodes $m$ and $n$ with $W$, concatenate column‚Äëwise to obtain $h_{m,n}$ <div style="margin-top: 0.5em;"></div>
  2. Linear projection with $b h_{m,n}$ <div style="margin-top: 0.5em;"></div>
  3. Activation ‚Üí softmax to compute attention score $\alpha$ between electrodes. <div style="margin-top: 0.5em;"></div>
  4. Vector $h'_m$ capturing spatial info for electrode $h'_m = \sum_{n \in \mathcal{N}(m)} \alpha_{mn} W h_n$ <div style="margin-top: 0.5em;"></div>
  5. Pool $h'$ and feed into task‚Äëspecific head.  

<small>‚ö†Ô∏è $b$ and $W$ are learned jointly across tasks.<br>
‚ö†Ô∏è Pooling method was not clarified
</small>


::right::

<div style="margin-top: 3em;"></div>

### Simple example

<div style="text-align: center;">
  <div style="margin-top: 0.5em;"></div> 

  ##### Red Edge: connection between used electrodes (F3 & P3)

```mermaid
graph LR
  C(("C3<br/>[0, 0, 0]<sup>T</sup>"))
  F(("F3<br/>[1, -0.3, 2]<sup>T</sup>"))
  P(("P3<br/>[-1, 4, 0]<sup>T</sup>"))

  C --- P
  F --- C
  F --- P

  %% ÂêÑ„É™„É≥„ÇØ„ÅÆ„Çπ„Çø„Ç§„É´ÊåáÂÆöÔºà0„Åã„ÇâÈ†ÜÁï™„Å´Ôºâ
  linkStyle 0 stroke:#dddddd,stroke-width:2px
  linkStyle 1 stroke:#dddddd,stroke-width:2px
  linkStyle 2 stroke:#e74c3c,stroke-width:4px
```

<div style="text-align:center;">
$$
\mathbf{b} \times
\underbrace{
\begin{bmatrix} W \times F_3\\ W \times P_3\end{bmatrix}
}_{h_{F_3,P_3}} \quad
- in \quad 1, 2
$$
</div>

<div style="text-align:center;">
$$
h'_{F_3} = \alpha_{F_3,F_3} W h_{F_3} + \alpha_{F_3,P_3} W h_{P_3} \quad
- in \quad 4
$$
</div>
</div>


---
layout: two-cols
---

## 3d. Datasets Used<div style="margin-top: 0.6em;"></div>
##### Preprocessed data: 37.5‚ÄØM samples ‚Üí 1‚ÄØB tokens

![](./pdf/hoge/table2.png)


<small>‚ö†Ô∏è The sample # counting and the tokenisation method are unclear.<br>(Prior work, BIOT, used frequency components as tokens)</small>

::right::

<div style="margin-top: 3em;"></div>

#### Dataset Overview<br>

<small>

| „Çø„Çπ„ÇØÂêç                      | „Çø„Çπ„ÇØÂÜÖÂÆπÊ¶ÇË¶Å + ‰æã                      |
| :---: | :---: |
| <span class="text-red-500 font-semibold">ER</span>; Emotion Recognition  | Classify emotions (joy, sadness, fear, etc.) |
| <span class="text-blue-500 font-semibold">MI</span>; Motor Imagery        | Classify imagined movement (left/right hand imaginary movement) |
| <span class="text-purple-500 font-semibold">MW</span>; Mental Workload      | Estimate cognitive load (low vs high, e.g, during rest vs subtraction task) |
| <span class="text-green-500 font-semibold">SS</span>; Sleep Stage          | Classify sleep stages (Wake/N1/N2/N3/REM) |
| <span class="text-yellow-500 font-semibold">CM</span>; Cross-Modality | Cross‚Äëmodal ERP tasks (classify categories of viewed images) |

</small>

---
layout: two-cols
---

## 3d. ML Models Used<div style="margin-top: 0.5em;"></div>
- EEGPT models & parameter sizes
<div style="text-align: center;">
  <img src="./pdf/hoge/table1.png" style="width: 80%;">
</div>

<div style="margin-top: 1em;"></div>


- Convergence of autoregressive pre‚Äëtraining ‚úîÔ∏è

<div style="position: relative; display: inline-block;">
  <img src="./pdf/hoge/fig4a.png" style="width: 60%;">
  <div class="text-xs text-gray-500" style="position: absolute; bottom: 5px; right: 100px;">
    Figure. 4a
  </div>
</div>


::right::

<div style="margin-top: 2em;"></div>

- Competing models trained per task

<small>

| **EEGNet** (2018)    | Depthwise + Separable CNN                         | BCI: P300, ERN, SMR, etc.                        |
| :---: | :---: | :---: |
| **TSception** (2022) | Multi‚Äëscale temporal CNN + asymmetric spatial CNN | Emotion (valence/arousal)                        |
| **Conformer** (2023) | 1D CNN + Self‚ÄëAttention                           | Motor imagery, emotion, etc.                     |
| **LGGNet** (2023)    | Local & Global GNN by functional regions          | Attention, fatigue, emotion                      |
| **BIOT** (2024)      | MAE √ó channel‚Äëwise tokenisation             | Pre‚Äëtraining ‚Üí seizure detection                 |
| **LaBraM** (2024)    | VQ tokeniser + MAE              | Cross‚Äëtask transfer: abnormal EEG, emotion, etc. |

</small>


---
layout: two-cols
---

## 4. Result 1: EEGPT Outperforms<div style="margin-top: 1em;"></div>

![](./pdf/hoge/table3.png)

::right::
## &nbsp;Specialised Models
<div style="text-align: center;">
  <p align="center">
    <img src="./pdf/hoge/fig1.png" style="width: 75%;">
  </p>
</div>
<div class="text-xs text-gray-500 mt-2 text-right">
Figure. 1
</div>

<div style="text-align: right;">
  <small>
    5 ~ 11% improvement in accuracy on average across the tasks<br>  
    <span class="text-red-500 font-semibold">ER</span> task: avg. 5% ‚Üë<br>
    <span class="text-green-500 font-semibold">SS</span> task: avg. 11% ‚Üë
  </small>
</div>



---

## 4. Result 2: Demonstration of Scaling Laws<div style="margin-top: 0.5em;"></div>

- Increasing model parameters improves performance

<div style="position: relative; display: inline-block;">
  <img src="./pdf/hoge/fig4b.png" style="width: 60%;">
  <div class="text-xs text-gray-500" style="position: absolute; bottom: 5px; right: 210px;">
    Figure. 4b
  </div>
</div>

- Increasing data size for AR pre‚Äëtraining further boosts performance ‚Üí suggests room for improvement

<div style="position: relative; display: inline-block;">
  <img src="./pdf/hoge/fig5.png" style="width: 90%;">
  <div class="text-xs text-gray-500" style="position: absolute; bottom: 5px; right: 20px;">
    Figure. 5
  </div>
</div>


---
layout: two-cols
---

## 4. Result 3: AR & TEG utility<div style="margin-top: 1em;"></div>

- Regarding pre‚Äëtraining
  - Compare MAE vs AR, the mainstream for EEG analysis using Transformers

<div style="margin-top: 0.75em;"></div>

<p align="center">
  <img src="./pdf/hoge/table4.png" style="width: 80%;">
</p>

<div style="margin-top: 1em;"></div>

- AR pre‚Äëtraining, which learns chronological dependency, achieves the higher performance  
- AR can replace conventional MAE for EEG analysis

::right::

<div style="margin-top: 3.7em;"></div>

- Regarding fine‚Äëtuning
  - Compare training per task vs joint training (share electrode graphs; TEG) across all tasks

<div style="margin-top: 0.75em;"></div>

<p align="center">
  <img src="./pdf/hoge/table5.png" style="width: 80%;">
</p>

<div style="margin-top: 1em;"></div>

- Joint learning consistently improves performance, even for tasks with limited data. i.e.,<br>task MW: 4K tokens, whereas task SS: 42K tokens
- The multi‚Äëtask setup of EEGPT is effective and promising for future EEG analysis.


---

## 4. Result 4: Learned AR Representations on Unseen Data<div style="margin-top: 0.5em;"></div>

- Dataset: DREAMER
  - Subjects: 23 participants
  - Task: emotion recognition
  - Labels: 4‚Äëclass (valence high/low √ó arousal high/low)
- Procedure
  - Feed EEG signals into ETE, take the last token per electrode
  - Mean‚Äëpool and visualise samples with t‚ÄëSNE

<div style="position: relative; display: inline-block;">
  <p align="center">
    <img src="./pdf/hoge/fig6.png" style="width: 70%;">
  </p>
  <div class="text-xs text-gray-500" style="position: absolute; bottom: 20px; right: 40px;">
    Figure. 6
  </div>
</div>


---

## üßµ Conclusion<div style="margin-top: 0.5em;"></div>
- Achieved creation of a **general EEG analysis model** ‚úîÔ∏è
- Confirmed in EEG the scaling laws and utility of AR, which are also reported in NLP field ‚úîÔ∏è
- **Multi‚Äëtask learning** via a shared electrode graph enhances performance ‚úîÔ∏è

<br><br>

## üßê Limitations<div style="margin-top: 0.5em;"></div>
1. **EEG tokens?**: The signal ‚Üí token conversion method is not specified.
1. **Positional encoding?**: Is positional encoding unnecessary for AR?
1. **Importance of TEG?**: If AR alone yields good latent representations (Fig.‚ÄØ6), how crucial is TEG for tasks?
1. **No proving**: Unclear which chronological domains are important for feature extraction and/or label classification.


---

# supplementary information üëâ


---

# See Full PDF<br><br>

[üìù View Full PDF](./pdf/hoge/EEGPT.pdf)

---
layout: two-cols
---

## SwiGLU<br>

![](./pdf/hoge/jcarlosroldan.com_swiglu.png)

<div class="text-xs text-gray-500 mt-2 text-right">
jcarlosroldan.com
</div>

::right::

## Leaky ReLU<br>

![](./pdf/hoge/keisan.casio.jp_leakyrelu.png)

<div class="text-xs text-gray-500 mt-2 text-right">
keisan.casio.jp
</div>

---
layout: two-cols
---

## Positional encoding matrix (PEM)<br>

![](./pdf/hoge/pe_viz_m1ke.org.png)

<div class="text-xs text-gray-500 mt-2 text-right">
M1KE BLOG
</div>

::right::

## Inner product of PEM<br>

![](./pdf/hoge/pe_naiseki_viz_m1ke.org.png)

<div class="text-xs text-gray-500 mt-2 text-right">
M1KE BLOG
</div>

